{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\stani\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.2.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (78.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\stani\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\stani\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\stani\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "     ---------------------------------------- 0.0/16.3 MB ? eta -:--:--\n",
      "     --------------------------- ----------- 11.3/16.3 MB 58.6 MB/s eta 0:00:01\n",
      "     --------------------------------------- 16.3/16.3 MB 44.5 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (2.2.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (2.2.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.51.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\stani\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\stani\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.0-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xgboost) (2.2.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\stani\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xgboost) (1.15.2)\n",
      "Downloading xgboost-3.0.0-py3-none-win_amd64.whl (150.0 MB)\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 7.6/150.0 MB 39.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 18.9/150.0 MB 47.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 31.2/150.0 MB 50.7 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 41.7/150.0 MB 50.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 55.3/150.0 MB 52.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 68.4/150.0 MB 53.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 81.8/150.0 MB 54.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 94.1/150.0 MB 55.6 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 104.9/150.0 MB 54.4 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 117.7/150.0 MB 54.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 129.8/150.0 MB 54.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 142.6/150.0 MB 55.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.9/150.0 MB 55.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.9/150.0 MB 55.7 MB/s eta 0:00:01\n",
      "   --------------------------------------- 150.0/150.0 MB 46.5 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#Installs\n",
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!python -m spacy download fr_core_news_sm\n",
    "!pip install scikit-learn\n",
    "!pip install imbalanced-learn\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install sentencepiece\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Requirements\n",
    "import spacy  \n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "import re  \n",
    "import nltk \n",
    "from nltk.corpus import stopwords  \n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import torch\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import du fichier \n",
    "\n",
    "file_path = os.path.join(os.getcwd(), \"20220913_offers.csv\")\n",
    "df = pd.read_csv(file_path, sep=\";\", low_memory=False)\n",
    "\n",
    "columns = [\"job_title\", \"contractType\", \"description_job\",\"description_profil\", \"description_entreprise\",\"entreprise_nom\"]\n",
    "df = df[columns]\n",
    "df['offre_emploi'] = df[['description_job', 'description_profil', 'description_entreprise']].apply(\n",
    "    lambda x: ' '.join(x.dropna()), axis=1\n",
    ")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"offre_emploi\"] = df[\"offre_emploi\"].str.lower()\n",
    "\n",
    "nltk.download('stopwords')  \n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Fonction de nettoyage avec lemmatisation\n",
    "def clean_and_lemmatize(text):\n",
    "    if isinstance(text, str):  \n",
    "        text = text.lower()  # Minuscule\n",
    "        text = re.sub(r'\\d+', '', text)  # Suppression des chiffres\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Suppression de la ponctuation\n",
    "        doc = nlp(text)  # Analyse du texte avec SpaCy\n",
    "        words = [token.lemma_ for token in doc if token.text not in stop_words]  # Lemmatisation + suppression stopwords\n",
    "        return \" \".join(words)  # Reconstituer le texte nettoyé\n",
    "    return \"\"\n",
    "\n",
    "df[\"offre_emploi\"] = df[\"offre_emploi\"].apply(clean_and_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#première classification : on regarde le nombre d'occurrences de chaque mot associé à chaque \n",
    "#catégorie, à partir de là on établit un score pour chaque catégorie et on assigne à une offre \n",
    "#d'emploi la catégorie pour laquelle le score est le plus élevé \n",
    "\n",
    "def classify_offer(text, keywords_dict):\n",
    "    scores = {category: 0 for category in keywords_dict}\n",
    "    \n",
    "    for category, keywords in keywords_dict.items():\n",
    "        scores[category] += sum(text.count(keyword) for keyword in keywords)\n",
    "    \n",
    "    return max(scores, key=scores.get) if max(scores.values()) > 0 else \"inconnu\"\n",
    "\n",
    "#dictionnaire avec mots clés :\n",
    "\n",
    "keywords_dict = {\n",
    "    \"Excellence, Leadership et Performance\": [\n",
    "        \"leader\", \"leadership\", \"lead\", \"diriger\", \"décider\", \"excellence\", \"exceller\", \"haut niveau\", \"prestigieux\", \"prestige\", \"réputation\", \"réputé\", \"certification\",\n",
    "        \"expert\", \"expertise\", \"compétence\", \"qualifié\", \"qualité\", \"performance\", \"performant\", \"responsabilité\"\n",
    "    ],\n",
    "    \"Accompagnement et Evolution Professionnelle\": [\n",
    "        \"accompagnement\", \"accompagner\", \"évolution\", \"évoluer\", \"formation\", \"formation continu\", \"développement\", \"développer\", \"carrière\", \"professionnel\", \"promotion\",\n",
    "        \"succès\", \"opportunité\", \"soutien\"\n",
    "    ],\n",
    "    \"Proximité et Ancrage Local\": [\n",
    "        \"proximité\", \"ancrage local\", \"lien\", \"relation client\", \"territoire\", \"local\", \"régional\",\n",
    "        \"implantation\", \"département\", \"acteur\", \"région\"\n",
    "    ],\n",
    "    \"Relations Humaines et Collectif\": [\n",
    "        \"esprit déquipe\", \"équipe\", \"familial\", \"convivialité\", \"convivial\", \"entraide\", \"partage\", \"solidarité\", \"respect\",\n",
    "        \"famille\", \"cohésion\"\n",
    "    ],\n",
    "    \"Engagement Social et Bien-être\": [\n",
    "        \"rse\", \"social\", \"citoyen\", \"planete\", \"durable\", \"développement durable\", \"inclusion\", \"handicap\", \"diversité\", \"impact social\",\n",
    "        \"bienêtre\", \"environnement\", \"santé\"\n",
    "    ]#,\n",
    "    #\"Innovation et Technologie\": [\n",
    "    #    \"innovation\", \"tech\", \"technologie\", \"transformation digital\", \"croissance\", \"expansion\", \"startup\", \"modernisation\", \"intelligence artificiel\", \"ia\", \"ai\", \"artificiel\", \"logiciel\",\n",
    "    #    \"inovant\", \"r&d\"\n",
    "    #]\n",
    "}\n",
    "\n",
    "df[\"valeur_dominante\"] = df[\"offre_emploi\"].apply(lambda x: classify_offer(x, keywords_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deuxième classification : on fait la même chose mais on accorde davantage de poids aux mots rares\n",
    "#car ils sont plus discriminants, en utilisant TF-IDF \n",
    "\n",
    "#étape 1 : calcul de la fréquence de chaque mot \n",
    "\n",
    "liste_valeurs = list(keywords_dict.values())\n",
    "liste_valeurs = [mot for sous_liste in liste_valeurs for mot in sous_liste]\n",
    "# On crée une liste avec tous nos mots clefs \n",
    "liste1 = []\n",
    "# On remplit une liste vide par les fréquences des mots clefs associés à liste_valeurs\n",
    "for mot in liste_valeurs:\n",
    "    i = 0\n",
    "    for k in df[\"offre_emploi\"]:\n",
    "        i += len(re.findall(r'\\b' + re.escape(mot) + r'\\b', k))\n",
    "    liste1+= [i]\n",
    "a=sum(liste1)\n",
    "liste2 = [x / a for x in liste1]\n",
    "print(liste2)\n",
    "print(liste_valeurs)\n",
    "\n",
    "#étape 2 : calcul de la fréquence inverse de chaque terme (mesure de l'importance du terme dans l'ensemble du corpus)\n",
    "\n",
    "N = len(df)\n",
    "idf_dict = {}\n",
    "for mot in liste_valeurs:\n",
    "    DF = 0\n",
    "    for k in df[\"offre_emploi\"]:\n",
    "        # Vérifier que k est une chaîne et que le mot apparaît dans l'offre\n",
    "        if isinstance(k, str) and re.search(r'\\b' + re.escape(mot) + r'\\b', k):\n",
    "            DF += 1\n",
    "    idf_dict[mot] = math.log(N / (1 + DF))\n",
    "\n",
    "def classify_offer2(offre): \n",
    "    scores = {category: 0 for category in keywords_dict}\n",
    "    \n",
    "    for category, keywords in keywords_dict.items():\n",
    "        scores[category] += sum(offre.count(keyword)*idf_dict.get(keyword, 0) for keyword in keywords)\n",
    "    \n",
    "    return max(scores, key=scores.get) if max(scores.values()) > 0 else \"inconnu\"\n",
    "\n",
    "\n",
    "df[\"valeur_dominante_2\"] = df[\"offre_emploi\"].apply(lambda x: classify_offer2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-means for 1st clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    entreprise_nom  \\\n",
      "0                 Triangle Intérim   \n",
      "1                          Abalone   \n",
      "2                   123webimmo.com   \n",
      "3  Fondation des Amis de l'Atelier   \n",
      "4                           WEETEC   \n",
      "\n",
      "                                     description_job  \\\n",
      "0  La société TRIANGLE, entreprise à taille humai...   \n",
      "1  Nous recherchons un BOUCHER INDUSTRIEL (H/F) E...   \n",
      "2  En tant qu’agent commercial immobilier au sein...   \n",
      "3  Au sein de l'équipe de Direction, vous partici...   \n",
      "4  Rattaché(e) au responsable d’affaire, vos prin...   \n",
      "\n",
      "                                job_title  \\\n",
      "0         Technicien de maintenance (H/F)   \n",
      "1                           Boucher (H/F)   \n",
      "2             Conseiller immobilier (H/F)   \n",
      "3  Cadre administratif et financier (H/F)   \n",
      "4                  Chef de chantier (H/F)   \n",
      "\n",
      "                                  description_profil  \\\n",
      "0  Vous avez des connaissances en maintenance ind...   \n",
      "1  De formation CAP Boucherie Travail physique, r...   \n",
      "2  Que vous soyez débutant(e) ou confirmé(e) dans...   \n",
      "3  Titulaire d’un Master en gestion, vous avez ac...   \n",
      "4  De formation électrotechnique ou électricité (...   \n",
      "\n",
      "                              description_entreprise  catégories  \\\n",
      "0    Chez TRIANGLE SOLUTIONS RH , nos liens sont ...         4.0   \n",
      "1    Créé en 1991 par François-Xavier Moutel, ABA...         1.0   \n",
      "2                                                NaN         1.0   \n",
      "3   123 webimmo . comest un réseau de 90 agences ...         5.0   \n",
      "4   La société CESA spécialisée dans les travaux ...         1.0   \n",
      "\n",
      "                                        offre_emploi  \n",
      "0  La société TRIANGLE, entreprise à taille humai...  \n",
      "1  Nous recherchons un BOUCHER INDUSTRIEL (H/F) E...  \n",
      "2  En tant qu’agent commercial immobilier au sein...  \n",
      "3  Au sein de l'équipe de Direction, vous partici...  \n",
      "4  Rattaché(e) au responsable d’affaire, vos prin...  \n",
      "Index(['entreprise_nom', 'description_job', 'job_title', 'description_profil',\n",
      "       'description_entreprise', 'catégories', 'offre_emploi'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#import du fichier \n",
    "\n",
    "file_path = os.path.join(os.getcwd(), \"700offers.200classified.csv\")\n",
    "df = pd.read_csv(file_path, sep=\",\", low_memory=False)\n",
    "\n",
    "df['offre_emploi'] = df[['description_job', 'description_profil', 'description_entreprise']].apply(\n",
    "    lambda x: ' '.join(x.dropna()), axis=1\n",
    ")\n",
    "\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\stani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#création du csv lematised700.csv (qu'on utilisera/modifiera)\n",
    "df[\"offre_emploi\"] = df[\"offre_emploi\"].str.lower()\n",
    "\n",
    "nltk.download('stopwords')  \n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Fonction de nettoyage avec lemmatisation\n",
    "def clean_and_lemmatize(text):\n",
    "    if isinstance(text, str):  \n",
    "        text = text.lower()  # Minuscule\n",
    "        text = re.sub(r'\\d+', '', text)  # Suppression des chiffres\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Suppression de la ponctuation\n",
    "        doc = nlp(text)  # Analyse du texte avec SpaCy\n",
    "        words = [token.lemma_ for token in doc if token.text not in stop_words]  # Lemmatisation + suppression stopwords\n",
    "        return \" \".join(words)  # Reconstituer le texte nettoyé\n",
    "    return \"\"\n",
    "\n",
    "df[\"offre_emploi\"] = df[\"offre_emploi\"].apply(clean_and_lemmatize)\n",
    "df.to_csv(\"lematised700.csv\", sep=\",\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Régression logistique en TF-IDF embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répartition des classes:\n",
      "catégories\n",
      "2    53\n",
      "1    49\n",
      "3    36\n",
      "4    31\n",
      "5    27\n",
      "Name: count, dtype: int64\n",
      "Validation croisée (5-fold) sur l'ensemble d'entraînement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores F1 Macro pour chaque fold : [0.18275862 0.17056277 0.17411765 0.18544061 0.1837037 ]\n",
      "F1 Macro moyen : 0.17931667100835458\n",
      "Rapport de classification :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.47      0.70      0.56        10\n",
      "           2       0.38      0.82      0.51        11\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.00      0.00      0.00         6\n",
      "           5       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.40        40\n",
      "   macro avg       0.17      0.30      0.21        40\n",
      "weighted avg       0.22      0.40      0.28        40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Filtrer pour conserver uniquement les lignes avec catégorie (excluant les NaN)\n",
    "df = df.dropna(subset=['catégories'])\n",
    "df['catégories'] = df['catégories'].astype(int)\n",
    "\n",
    "# Vérification de la répartition des classes\n",
    "print(\"Répartition des classes:\")\n",
    "print(df['catégories'].value_counts())\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test (80%/20% avec stratification)\n",
    "X = df[\"offre_emploi\"]\n",
    "y = df[\"catégories\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- Création du pipeline ---\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=500))\n",
    "])\n",
    "\n",
    "# --- Validation croisée ---\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(\"Validation croisée (5-fold) sur l'ensemble d'entraînement...\")\n",
    "scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='f1_macro')\n",
    "print(\"Scores F1 Macro pour chaque fold :\", scores)\n",
    "print(\"F1 Macro moyen :\", scores.mean())\n",
    "\n",
    "# --- Entraînement final sur l'ensemble d'entraînement ---\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Affichage du rapport de classification\n",
    "print(\"Rapport de classification :\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Résultats\n",
    "\n",
    "Répartition des classes:\n",
    "catégories\n",
    "2    53\n",
    "1    49\n",
    "3    36\n",
    "4    31\n",
    "5    27\n",
    "\n",
    "Régression logistique; vectorisation en TF-IDF; sur 200 lignes (données labelisées) : \n",
    "\n",
    "Rapport de classification :\n",
    "\n",
    "              precision  ;  recall ; f1-score  ; support              \n",
    "           1       0.47      0.70      0.56        10\n",
    "           2       0.38      0.82      0.51        11\n",
    "           3       0.00      0.00      0.00         7\n",
    "           4       0.00      0.00      0.00         6\n",
    "           5       0.00      0.00      0.00         6\n",
    "\n",
    "    accuracy                           0.40        40\n",
    "    macro avg       0.17      0.30      0.21        40\n",
    "    weighted avg       0.22      0.40      0.28        40\n",
    "\n",
    "Commentaire :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Régression logistique en CamemBERT embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation croisée (5-fold) sur l'ensemble d'entraînement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores F1 Macro pour chaque fold : [0.29812994 0.21238095 0.35288221 0.45320856 0.29482402]\n",
      "F1 Macro moyen : 0.3222851341685639\n",
      "Entraînement final du modèle sur tout l'ensemble d'entraînement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camembert-based Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.47      0.70      0.56        10\n",
      "           2       0.46      0.55      0.50        11\n",
      "           3       0.14      0.14      0.14         7\n",
      "           4       0.00      0.00      0.00         6\n",
      "           5       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.35        40\n",
      "   macro avg       0.21      0.28      0.24        40\n",
      "weighted avg       0.27      0.35      0.30        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "\n",
    "# Garder seulement les lignes où 'catégories' n'est pas NaN et convertir en int\n",
    "df = df.dropna(subset=['catégories'])\n",
    "df['catégories'] = df['catégories'].astype(int)\n",
    "\n",
    "# Séparation en ensemble d'entraînement et de test (80%/20% en stratifiant)\n",
    "X = df[\"offre_emploi\"]\n",
    "y = df[\"catégories\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- Définition du transformeur Camembert ---\n",
    "class CamembertVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformeur scikit-learn qui convertit un texte en un vecteur d'embedding\n",
    "    à l'aide du modèle Camembert.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='camembert-base', device='cpu', max_length=512):\n",
    "        self.model_name = model_name  # Nécessaire pour le clonage dans CV\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = CamembertTokenizer.from_pretrained(model_name)\n",
    "        self.model = CamembertModel.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        embeddings = []\n",
    "        for text in X:\n",
    "            # Tokenisation avec troncature et padding\n",
    "            inputs = self.tokenizer(\n",
    "                text,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "            # Déplacer les tenseurs sur le device (CPU ou GPU)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            # Extraction de la représentation du token [CLS] (premier token)\n",
    "            emb = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "            embeddings.append(emb)\n",
    "        return np.array(embeddings)\n",
    "\n",
    "# Définir le device (utiliser \"cuda\" si disponible, sinon \"cpu\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# --- Construction du pipeline Camembert + Régression Logistique ---\n",
    "pipeline_camembert = Pipeline([\n",
    "    ('camembert', CamembertVectorizer(device=device)),\n",
    "    ('clf', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=500))\n",
    "])\n",
    "\n",
    "# --- Validation croisée ---\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(\"Validation croisée (5-fold) sur l'ensemble d'entraînement...\")\n",
    "scores = cross_val_score(pipeline_camembert, X_train, y_train, cv=cv, scoring='f1_macro')\n",
    "print(\"Scores F1 Macro pour chaque fold :\", scores)\n",
    "print(\"F1 Macro moyen :\", scores.mean())\n",
    "\n",
    "# --- Entraînement final ---\n",
    "print(\"Entraînement final du modèle sur tout l'ensemble d'entraînement...\")\n",
    "pipeline_camembert.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test et affichage du rapport de classification\n",
    "y_pred = pipeline_camembert.predict(X_test)\n",
    "print(\"Camembert-based Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Résultats\n",
    "\n",
    "Régression logistique ; embbeding en Camembert ; sur 200 lignes (données labelisées)\n",
    "\n",
    "Camembert-based Classification Report:\n",
    "\n",
    "              precision  ;  recall ; f1-score ;  support\n",
    "           1       0.47      0.70      0.56        10\n",
    "           2       0.46      0.55      0.50        11\n",
    "           3       0.14      0.14      0.14         7\n",
    "           4       0.00      0.00      0.00         6\n",
    "           5       0.00      0.00      0.00         6\n",
    "\n",
    "    accuracy                           0.35        40\n",
    "    macro avg       0.21      0.28      0.24        40\n",
    "    weighted avg       0.27      0.35      0.30        40\n",
    "\n",
    "Commentaire :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation croisée (5-fold) sur l'ensemble d'entraînement...\n",
      "Scores F1 Macro pour chaque fold : [0.17649123 0.26       0.37054454 0.29364706 0.29090909]\n",
      "F1 Macro moyen : 0.27831838419578353\n",
      "Entraînement final du modèle sur tout l'ensemble d'entraînement...\n",
      "Random Forest + Camembert Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.90      0.67        10\n",
      "           2       0.43      0.55      0.48        11\n",
      "           3       0.33      0.29      0.31         7\n",
      "           4       0.00      0.00      0.00         6\n",
      "           5       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.42        40\n",
      "   macro avg       0.26      0.35      0.29        40\n",
      "weighted avg       0.31      0.42      0.35        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "from imblearn.pipeline import Pipeline  # Pipeline compatible avec imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "df = df.dropna(subset=['catégories'])\n",
    "df['catégories'] = df['catégories'].astype(int)\n",
    "\n",
    "# Séparation en ensemble d'entraînement et de test (stratification)\n",
    "X = df[\"offre_emploi\"]\n",
    "y = df[\"catégories\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=42, stratify=y)\n",
    "\n",
    "# --- Définition du transformeur Camembert ---\n",
    "class CamembertVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformeur qui convertit un texte en un vecteur d'embedding\n",
    "    à l'aide du modèle Camembert.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='camembert-base', device='cpu', max_length=512):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = CamembertTokenizer.from_pretrained(model_name)\n",
    "        self.model = CamembertModel.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        embeddings = []\n",
    "        for text in X:\n",
    "            inputs = self.tokenizer(\n",
    "                text,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            # Extraction de la représentation du token [CLS] (premier token)\n",
    "            emb = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "            embeddings.append(emb)\n",
    "        return np.array(embeddings)\n",
    "\n",
    "# --- Wrapper pour oversampling ---\n",
    "class OversampleTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Wrapper qui intègre un oversampler et fournit les méthodes fit et transform.\n",
    "    Pendant l'entraînement, il applique fit_resample.\n",
    "    En phase de prédiction, il renvoie simplement les données X.\n",
    "    \"\"\"\n",
    "    def __init__(self, sampler):\n",
    "        self.sampler = sampler\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.X_res_, self.y_res_ = self.sampler.fit_resample(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "\n",
    "# Définir le device (GPU si disponible)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# --- Construction du pipeline avec oversampling et Random Forest ---\n",
    "pipeline_camembert_rf = Pipeline([\n",
    "    ('camembert', CamembertVectorizer(device=device)),\n",
    "    ('oversample', OversampleTransformer(RandomOverSampler(random_state=42))),\n",
    "    ('clf', RandomForestClassifier(n_estimators=300,\n",
    "                                   max_depth=10,\n",
    "                                   random_state=42))\n",
    "])\n",
    "\n",
    "# --- Validation croisée (5-fold) ---\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(\"Validation croisée (5-fold) sur l'ensemble d'entraînement...\")\n",
    "scores = cross_val_score(pipeline_camembert_rf, X_train, y_train, cv=cv, scoring='f1_macro')\n",
    "print(\"Scores F1 Macro pour chaque fold :\", scores)\n",
    "print(\"F1 Macro moyen :\", scores.mean())\n",
    "\n",
    "# --- Entraînement final sur tout l'ensemble d'entraînement ---\n",
    "print(\"Entraînement final du modèle sur tout l'ensemble d'entraînement...\")\n",
    "pipeline_camembert_rf.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred_rf = pipeline_camembert_rf.predict(X_test)\n",
    "\n",
    "# Affichage du rapport de classification final\n",
    "print(\"Random Forest + Camembert Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Résultats\n",
    "\n",
    "Random Forest + Camembert Classification Report:\n",
    "\n",
    "              precision   ; recall ; f1-score  ; support\n",
    "           1       0.53      0.90      0.67        10\n",
    "           2       0.43      0.55      0.48        11\n",
    "           3       0.33      0.29      0.31         7\n",
    "           4       0.00      0.00      0.00         6\n",
    "           5       0.00      0.00      0.00         6\n",
    "\n",
    "    accuracy                           0.42        40\n",
    "    macro avg       0.26      0.35      0.29        40\n",
    "    weighted avg       0.31      0.42      0.35        40\n",
    "\n",
    "Commentaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation croisée (5-fold)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:37:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:39:20] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:41:18] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:43:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:45:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores F1 Macro pour chaque fold: [0.25367965 0.30439394 0.34373434 0.32676213 0.23333333]\n",
      "F1 Macro moyen: 0.29238067780173044\n",
      "Entraînement final sur tout l'ensemble d'entraînement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:47:38] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Camembert + XGBoost + Oversampling):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.60      0.55        10\n",
      "           1       0.31      0.36      0.33        11\n",
      "           2       0.20      0.14      0.17         7\n",
      "           3       0.33      0.33      0.33         6\n",
      "           4       0.25      0.17      0.20         6\n",
      "\n",
      "    accuracy                           0.35        40\n",
      "   macro avg       0.32      0.32      0.32        40\n",
      "weighted avg       0.33      0.35      0.34        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "# Chargement et filtrage des données\n",
    "df = df.dropna(subset=['catégories'])\n",
    "df['catégories'] = df['catégories'].astype(int)\n",
    "\n",
    "X = df[\"offre_emploi\"]\n",
    "y = df[\"catégories\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Camembert Vectorizer\n",
    "class CamembertVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name='camembert-base', device='cpu', max_length=512):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = CamembertTokenizer.from_pretrained(model_name)\n",
    "        self.model = CamembertModel.from_pretrained(model_name).to(device).eval()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        embeddings = []\n",
    "        for text in X:\n",
    "            inputs = self.tokenizer(\n",
    "                text, return_tensors='pt', truncation=True,\n",
    "                padding='max_length', max_length=self.max_length\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            emb = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "            embeddings.append(emb)\n",
    "        return np.array(embeddings)\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Correction des classes pour commencer à 0\n",
    "y_train_adj = y_train - 1\n",
    "y_test_adj = y_test - 1\n",
    "\n",
    "# Pipeline avec Oversampling + XGBoost\n",
    "pipeline_camembert_xgb = Pipeline([\n",
    "    ('camembert', CamembertVectorizer(device=device)),\n",
    "    ('oversample', RandomOverSampler(random_state=42)),\n",
    "    ('clf', XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='multi:softprob',\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Validation croisée stratifiée\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(\"Validation croisée (5-fold)...\")\n",
    "scores = cross_val_score(pipeline_camembert_xgb, X_train, y_train_adj, cv=cv, scoring='f1_macro')\n",
    "print(\"Scores F1 Macro pour chaque fold:\", scores)\n",
    "print(\"F1 Macro moyen:\", scores.mean())\n",
    "\n",
    "# Entraînement final et évaluation\n",
    "print(\"Entraînement final sur tout l'ensemble d'entraînement...\")\n",
    "pipeline_camembert_xgb.fit(X_train, y_train_adj)\n",
    "\n",
    "y_pred = pipeline_camembert_xgb.predict(X_test)\n",
    "print(\"Classification Report (Camembert + XGBoost + Oversampling):\")\n",
    "print(classification_report(y_test_adj, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Résultats :\n",
    "\n",
    "Classification Report (Camembert + XGBoost + Oversampling):\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "           0       0.50      0.60      0.55        10\n",
    "           1       0.31      0.36      0.33        11\n",
    "           2       0.20      0.14      0.17         7\n",
    "           3       0.33      0.33      0.33         6\n",
    "           4       0.25      0.17      0.20         6\n",
    "\n",
    "         accuracy                           0.35        40\n",
    "         macro avg       0.32      0.32      0.32        40\n",
    "         weighted avg       0.33      0.35      0.34        40\n",
    "\n",
    "Commentaire :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) SVM (kernel RBF, C=1.0, gamma réglé sur 'scale', et une graine pour la reproductibilité.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du modèle avec Camembert + SVM + Oversampling...\n",
      "SVM + Camembert Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.39      0.70      0.50        10\n",
      "           2       0.33      0.45      0.38        11\n",
      "           3       0.20      0.14      0.17         7\n",
      "           4       0.00      0.00      0.00         6\n",
      "           5       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.33        40\n",
      "   macro avg       0.18      0.26      0.21        40\n",
      "weighted avg       0.22      0.33      0.26        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "from imblearn.pipeline import Pipeline   # Assure-toi d'utiliser imblearn.pipeline.Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Garder uniquement les lignes où 'catégories' n'est pas NaN et convertir en int\n",
    "df = df.dropna(subset=['catégories'])\n",
    "df['catégories'] = df['catégories'].astype(int)\n",
    "\n",
    "# Séparation en ensemble d'entraînement et de test (80%/20% en stratifiant)\n",
    "X = df[\"offre_emploi\"]\n",
    "y = df[\"catégories\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- Définition du transformeur Camembert ---\n",
    "class CamembertVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformeur qui convertit un texte en un vecteur d'embedding\n",
    "    à l'aide du modèle Camembert.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='camembert-base', device='cpu', max_length=512):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = CamembertTokenizer.from_pretrained(model_name)\n",
    "        self.model = CamembertModel.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        embeddings = []\n",
    "        for text in X:\n",
    "            inputs = self.tokenizer(\n",
    "                text,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            # Extraction de la représentation du token [CLS] (premier token)\n",
    "            emb = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "            embeddings.append(emb)\n",
    "        return np.array(embeddings)\n",
    "\n",
    "# --- Wrapper pour oversampling ---\n",
    "class OversampleTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Wrapper pour intégrer un oversampler dans le pipeline.\n",
    "    En phase d'entraînement, il applique fit_resample,\n",
    "    et en phase de prédiction, il renvoie simplement les données X.\n",
    "    \"\"\"\n",
    "    def __init__(self, sampler):\n",
    "        self.sampler = sampler\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.X_res_, self.y_res_ = self.sampler.fit_resample(X, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X\n",
    "\n",
    "# Définir le device (GPU si disponible, sinon CPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# --- Construction du pipeline avec oversampling et SVM ---\n",
    "# Le pipeline est constitué de :\n",
    "# 1. CamembertVectorizer pour obtenir les embeddings du texte\n",
    "# 2. Oversampling sur le jeu d'entraînement (RandomOverSampler)\n",
    "# 3. StandardScaler pour normaliser les features, essentiel pour le SVM\n",
    "# 4. SVC avec kernel RBF et des hyperparamètres paramétrés sans opti\n",
    "pipeline_camembert_svm = Pipeline([\n",
    "    ('camembert', CamembertVectorizer(device=device)),\n",
    "    ('oversample', OversampleTransformer(RandomOverSampler(random_state=42))),\n",
    "    ('scaler', StandardScaler()),  # Normalisation des features pour SVM\n",
    "    ('clf', SVC(\n",
    "        kernel='rbf',         # Noyau radial\n",
    "        C=1.0,                # Contrôle de la marge\n",
    "        gamma='scale',        # Paramètre du kernel\n",
    "        probability=True,     # Optionnel : permet de prédire des probabilités\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Entraînement\n",
    "print(\"Entraînement du modèle avec Camembert + SVM + Oversampling...\")\n",
    "pipeline_camembert_svm.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur le jeu de test et affichage du rapport de classification\n",
    "y_pred_svm = pipeline_camembert_svm.predict(X_test)\n",
    "print(\"SVM + Camembert Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Résultats :\n",
    "\n",
    "SVM + Camembert Classification Report:\n",
    "\n",
    "              precision  ;  recall ; f1-score ;  support\n",
    "           1       0.39      0.70      0.50        10\n",
    "           2       0.33      0.45      0.38        11\n",
    "           3       0.20      0.14      0.17         7\n",
    "           4       0.00      0.00      0.00         6\n",
    "           5       0.00      0.00      0.00         6\n",
    "\n",
    "    accuracy                           0.33        40\n",
    "    macro avg       0.18      0.26      0.21        40\n",
    "    weighted avg       0.22      0.33      0.26        40\n",
    "\n",
    "Commentaire :"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
