{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installs\n",
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!python -m spacy download fr_core_news_sm\n",
    "!pip install scikit-learn\n",
    "!pip install imbalanced-learn\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install sentencepiece\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Requirements\n",
    "import spacy  \n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "import re  \n",
    "import nltk \n",
    "from nltk.corpus import stopwords  \n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import torch\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import du fichier \n",
    "\n",
    "file_path = os.path.join(os.getcwd(), \"20220913_offers.csv\")\n",
    "df = pd.read_csv(file_path, sep=\";\", low_memory=False)\n",
    "\n",
    "columns = [\"job_title\", \"contractType\", \"description_job\",\"description_profil\", \"description_entreprise\",\"entreprise_nom\"]\n",
    "df = df[columns]\n",
    "df['offre_emploi'] = df[['description_job', 'description_profil', 'description_entreprise']].apply(\n",
    "    lambda x: ' '.join(x.dropna()), axis=1\n",
    ")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"offre_emploi\"] = df[\"offre_emploi\"].str.lower()\n",
    "\n",
    "nltk.download('stopwords')  \n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Fonction de nettoyage avec lemmatisation\n",
    "def clean_and_lemmatize(text):\n",
    "    if isinstance(text, str):  \n",
    "        text = text.lower()  # Minuscule\n",
    "        text = re.sub(r'\\d+', '', text)  # Suppression des chiffres\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Suppression de la ponctuation\n",
    "        doc = nlp(text)  # Analyse du texte avec SpaCy\n",
    "        words = [token.lemma_ for token in doc if token.text not in stop_words]  # Lemmatisation + suppression stopwords\n",
    "        return \" \".join(words)  # Reconstituer le texte nettoyé\n",
    "    return \"\"\n",
    "\n",
    "df[\"offre_emploi\"] = df[\"offre_emploi\"].apply(clean_and_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#première classification : on regarde le nombre d'occurrences de chaque mot associé à chaque \n",
    "#catégorie, à partir de là on établit un score pour chaque catégorie et on assigne à une offre \n",
    "#d'emploi la catégorie pour laquelle le score est le plus élevé \n",
    "\n",
    "def classify_offer(text, keywords_dict):\n",
    "    scores = {category: 0 for category in keywords_dict}\n",
    "    \n",
    "    for category, keywords in keywords_dict.items():\n",
    "        scores[category] += sum(text.count(keyword) for keyword in keywords)\n",
    "    \n",
    "    return max(scores, key=scores.get) if max(scores.values()) > 0 else \"inconnu\"\n",
    "\n",
    "#dictionnaire avec mots clés :\n",
    "\n",
    "keywords_dict = {\n",
    "    \"Excellence, Leadership et Performance\": [\n",
    "        \"leader\", \"leadership\", \"lead\", \"diriger\", \"décider\", \"excellence\", \"exceller\", \"haut niveau\", \"prestigieux\", \"prestige\", \"réputation\", \"réputé\", \"certification\",\n",
    "        \"expert\", \"expertise\", \"compétence\", \"qualifié\", \"qualité\", \"performance\", \"performant\", \"responsabilité\"\n",
    "    ],\n",
    "    \"Accompagnement et Evolution Professionnelle\": [\n",
    "        \"accompagnement\", \"accompagner\", \"évolution\", \"évoluer\", \"formation\", \"formation continu\", \"développement\", \"développer\", \"carrière\", \"professionnel\", \"promotion\",\n",
    "        \"succès\", \"opportunité\", \"soutien\"\n",
    "    ],\n",
    "    \"Proximité et Ancrage Local\": [\n",
    "        \"proximité\", \"ancrage local\", \"lien\", \"relation client\", \"territoire\", \"local\", \"régional\",\n",
    "        \"implantation\", \"département\", \"acteur\", \"région\"\n",
    "    ],\n",
    "    \"Relations Humaines et Collectif\": [\n",
    "        \"esprit déquipe\", \"équipe\", \"familial\", \"convivialité\", \"convivial\", \"entraide\", \"partage\", \"solidarité\", \"respect\",\n",
    "        \"famille\", \"cohésion\"\n",
    "    ],\n",
    "    \"Engagement Social et Bien-être\": [\n",
    "        \"rse\", \"social\", \"citoyen\", \"planete\", \"durable\", \"développement durable\", \"inclusion\", \"handicap\", \"diversité\", \"impact social\",\n",
    "        \"bienêtre\", \"environnement\", \"santé\"\n",
    "    ]#,\n",
    "    #\"Innovation et Technologie\": [\n",
    "    #    \"innovation\", \"tech\", \"technologie\", \"transformation digital\", \"croissance\", \"expansion\", \"startup\", \"modernisation\", \"intelligence artificiel\", \"ia\", \"ai\", \"artificiel\", \"logiciel\",\n",
    "    #    \"inovant\", \"r&d\"\n",
    "    #]\n",
    "}\n",
    "\n",
    "df[\"valeur_dominante\"] = df[\"offre_emploi\"].apply(lambda x: classify_offer(x, keywords_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deuxième classification : on fait la même chose mais on accorde davantage de poids aux mots rares\n",
    "#car ils sont plus discriminants, en utilisant TF-IDF \n",
    "\n",
    "#étape 1 : calcul de la fréquence de chaque mot \n",
    "\n",
    "liste_valeurs = list(keywords_dict.values())\n",
    "liste_valeurs = [mot for sous_liste in liste_valeurs for mot in sous_liste]\n",
    "# On crée une liste avec tous nos mots clefs \n",
    "liste1 = []\n",
    "# On remplit une liste vide par les fréquences des mots clefs associés à liste_valeurs\n",
    "for mot in liste_valeurs:\n",
    "    i = 0\n",
    "    for k in df[\"offre_emploi\"]:\n",
    "        i += len(re.findall(r'\\b' + re.escape(mot) + r'\\b', k))\n",
    "    liste1+= [i]\n",
    "a=sum(liste1)\n",
    "liste2 = [x / a for x in liste1]\n",
    "print(liste2)\n",
    "print(liste_valeurs)\n",
    "\n",
    "#étape 2 : calcul de la fréquence inverse de chaque terme (mesure de l'importance du terme dans l'ensemble du corpus)\n",
    "\n",
    "N = len(df)\n",
    "idf_dict = {}\n",
    "for mot in liste_valeurs:\n",
    "    DF = 0\n",
    "    for k in df[\"offre_emploi\"]:\n",
    "        # Vérifier que k est une chaîne et que le mot apparaît dans l'offre\n",
    "        if isinstance(k, str) and re.search(r'\\b' + re.escape(mot) + r'\\b', k):\n",
    "            DF += 1\n",
    "    idf_dict[mot] = math.log(N / (1 + DF))\n",
    "\n",
    "def classify_offer2(offre): \n",
    "    scores = {category: 0 for category in keywords_dict}\n",
    "    \n",
    "    for category, keywords in keywords_dict.items():\n",
    "        scores[category] += sum(offre.count(keyword)*idf_dict.get(keyword, 0) for keyword in keywords)\n",
    "    \n",
    "    return max(scores, key=scores.get) if max(scores.values()) > 0 else \"inconnu\"\n",
    "\n",
    "\n",
    "df[\"valeur_dominante_2\"] = df[\"offre_emploi\"].apply(lambda x: classify_offer2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-means for 1st clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    entreprise_nom  \\\n",
      "0                 Triangle Intérim   \n",
      "1                          Abalone   \n",
      "2                   123webimmo.com   \n",
      "3  Fondation des Amis de l'Atelier   \n",
      "4                           WEETEC   \n",
      "\n",
      "                                     description_job  \\\n",
      "0  La société TRIANGLE, entreprise à taille humai...   \n",
      "1  Nous recherchons un BOUCHER INDUSTRIEL (H/F) E...   \n",
      "2  En tant qu’agent commercial immobilier au sein...   \n",
      "3  Au sein de l'équipe de Direction, vous partici...   \n",
      "4  Rattaché(e) au responsable d’affaire, vos prin...   \n",
      "\n",
      "                                job_title  \\\n",
      "0         Technicien de maintenance (H/F)   \n",
      "1                           Boucher (H/F)   \n",
      "2             Conseiller immobilier (H/F)   \n",
      "3  Cadre administratif et financier (H/F)   \n",
      "4                  Chef de chantier (H/F)   \n",
      "\n",
      "                                  description_profil  \\\n",
      "0  Vous avez des connaissances en maintenance ind...   \n",
      "1  De formation CAP Boucherie Travail physique, r...   \n",
      "2  Que vous soyez débutant(e) ou confirmé(e) dans...   \n",
      "3  Titulaire d’un Master en gestion, vous avez ac...   \n",
      "4  De formation électrotechnique ou électricité (...   \n",
      "\n",
      "                              description_entreprise  catégories  \\\n",
      "0    Chez TRIANGLE SOLUTIONS RH , nos liens sont ...         4.0   \n",
      "1    Créé en 1991 par François-Xavier Moutel, ABA...         1.0   \n",
      "2                                                NaN         1.0   \n",
      "3   123 webimmo . comest un réseau de 90 agences ...         5.0   \n",
      "4   La société CESA spécialisée dans les travaux ...         1.0   \n",
      "\n",
      "                                        offre_emploi  \n",
      "0  La société TRIANGLE, entreprise à taille humai...  \n",
      "1  Nous recherchons un BOUCHER INDUSTRIEL (H/F) E...  \n",
      "2  En tant qu’agent commercial immobilier au sein...  \n",
      "3  Au sein de l'équipe de Direction, vous partici...  \n",
      "4  Rattaché(e) au responsable d’affaire, vos prin...  \n",
      "Index(['entreprise_nom', 'description_job', 'job_title', 'description_profil',\n",
      "       'description_entreprise', 'catégories', 'offre_emploi'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#import du fichier \n",
    "\n",
    "file_path = os.path.join(os.getcwd(), \"700offers.200classified.csv\")\n",
    "df = pd.read_csv(file_path, sep=\",\", low_memory=False)\n",
    "\n",
    "df['offre_emploi'] = df[['description_job', 'description_profil', 'description_entreprise']].apply(\n",
    "    lambda x: ' '.join(x.dropna()), axis=1\n",
    ")\n",
    "\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\stani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#création du csv lematised700.csv (qu'on utilisera/modifiera)\n",
    "df[\"offre_emploi\"] = df[\"offre_emploi\"].str.lower()\n",
    "\n",
    "nltk.download('stopwords')  \n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Fonction de nettoyage avec lemmatisation\n",
    "def clean_and_lemmatize(text):\n",
    "    if isinstance(text, str):  \n",
    "        text = text.lower()  # Minuscule\n",
    "        text = re.sub(r'\\d+', '', text)  # Suppression des chiffres\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Suppression de la ponctuation\n",
    "        doc = nlp(text)  # Analyse du texte avec SpaCy\n",
    "        words = [token.lemma_ for token in doc if token.text not in stop_words]  # Lemmatisation + suppression stopwords\n",
    "        return \" \".join(words)  # Reconstituer le texte nettoyé\n",
    "    return \"\"\n",
    "\n",
    "df[\"offre_emploi\"] = df[\"offre_emploi\"].apply(clean_and_lemmatize)\n",
    "df.to_csv(\"lematised700.csv\", sep=\",\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Régression logistique en TF-IDF embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répartition des classes:\n",
      "catégories\n",
      "2    53\n",
      "1    49\n",
      "3    36\n",
      "4    31\n",
      "5    27\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rapport de classification :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.47      0.70      0.56        10\n",
      "           2       0.38      0.82      0.51        11\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.00      0.00      0.00         6\n",
      "           5       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.40        40\n",
      "   macro avg       0.17      0.30      0.21        40\n",
      "weighted avg       0.22      0.40      0.28        40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Vérification de l'existence des catégories et filtrer pour conserver uniquement les offres classées (ne pas prendre en compte les Nan dans la colonne catégorie)\n",
    "df = df.dropna(subset=['catégories'])\n",
    "# On met en int pour que ca soit plus pratique\n",
    "df['catégories'] = df['catégories'].astype(int)\n",
    "\n",
    "# La colonne 'catégories' contient les classes manuelles (valeurs de 1 à 5)\n",
    "# Affichage de la distribution des classes pour vérifier\n",
    "print(\"Répartition des classes:\")\n",
    "print(df['catégories'].value_counts())\n",
    "\n",
    "# Séparation des données en ensemble d'entraînement et de test (80% train, 20% test)\n",
    "X = df[\"offre_emploi\"]\n",
    "y = df[\"catégories\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Création d'un pipeline de classification avec TF-IDF et régression logistique multinomiale\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=500))\n",
    "])\n",
    "\n",
    "# Entraînement du modèle\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Affichage du rapport de classification\n",
    "print(\"Rapport de classification :\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Résultats\n",
    "\n",
    "Répartition des classes:\n",
    "catégories\n",
    "2    53\n",
    "1    49\n",
    "3    36\n",
    "4    31\n",
    "5    27\n",
    "\n",
    "Régression logistique; vectorisation en TF-IDF; sur 200 lignes (données labelisées) : \n",
    "\n",
    "Rapport de classification :\n",
    "              precision  ;  recall ; f1-score  ; support\n",
    "\n",
    "           1       0.47      0.70      0.56        10\n",
    "           2       0.38      0.82      0.51        11\n",
    "           3       0.00      0.00      0.00         7\n",
    "           4       0.00      0.00      0.00         6\n",
    "           5       0.00      0.00      0.00         6\n",
    "\n",
    "    accuracy                           0.40        40\n",
    "   macro avg       0.17      0.30      0.21        40\n",
    "weighted avg       0.22      0.40      0.28        40\n",
    "\n",
    "Commentaire :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Régression logistique en CamemBERT embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répartition des classes:\n",
      "catégories\n",
      "2    53\n",
      "1    49\n",
      "3    36\n",
      "4    31\n",
      "5    27\n",
      "Name: count, dtype: int64\n",
      "Entraînement du modèle avec Camembert vectorization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camembert-based Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.47      0.70      0.56        10\n",
      "           2       0.46      0.55      0.50        11\n",
      "           3       0.14      0.14      0.14         7\n",
      "           4       0.00      0.00      0.00         6\n",
      "           5       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.35        40\n",
      "   macro avg       0.21      0.28      0.24        40\n",
      "weighted avg       0.27      0.35      0.30        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "# Filtrer pour ne conserver que les lignes où 'catégories' n'est pas NaN (les 200 offres classées)\n",
    "df = df.dropna(subset=['catégories'])\n",
    "# Convertir en int si besoin (valeurs attendues de 1 à 5)\n",
    "df['catégories'] = df['catégories'].astype(int)\n",
    "\n",
    "print(\"Répartition des classes:\")\n",
    "print(df['catégories'].value_counts())\n",
    "\n",
    "# Séparation en train et test (80%/20% avec stratification)\n",
    "X = df[\"offre_emploi\"]\n",
    "y = df[\"catégories\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "#--- Définition du transformeur Camembert ---\n",
    "\n",
    "class CamembertVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformeur scikit-learn qui convertit un texte en un vecteur d'embedding\n",
    "    à l'aide du modèle Camembert.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='camembert-base', device='cpu', max_length=512):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = CamembertTokenizer.from_pretrained(model_name)\n",
    "        self.model = CamembertModel.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        embeddings = []\n",
    "        for text in X:\n",
    "            # Tokenisation avec troncature et padding\n",
    "            inputs = self.tokenizer(\n",
    "                text,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "            # Déplacer les tenseurs sur le device (CPU ou GPU)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            # Extraction de la représentation du token [CLS] (premier token)\n",
    "            emb = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "            embeddings.append(emb)\n",
    "        return np.array(embeddings)\n",
    "\n",
    "# Définition du device : utiliser \"cuda\" si un GPU est disponible\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#--- Construction du pipeline Camembert ---\n",
    "pipeline_camembert = Pipeline([\n",
    "    ('camembert', CamembertVectorizer(device=device)),\n",
    "    ('clf', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=500))\n",
    "])\n",
    "\n",
    "# Entraînement du modèle\n",
    "print(\"Entraînement du modèle avec Camembert vectorization...\")\n",
    "pipeline_camembert.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test et affichage du rapport de classification\n",
    "y_pred = pipeline_camembert.predict(X_test)\n",
    "print(\"Camembert-based Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Résultats\n",
    "\n",
    "Régression logistique ; embbeding en Camembert ; sur 200 lignes (données labelisées)\n",
    "\n",
    "Camembert-based Classification Report:\n",
    "              precision  ;  recall ; f1-score ;  support\n",
    "\n",
    "           1       0.47      0.70      0.56        10\n",
    "           2       0.46      0.55      0.50        11\n",
    "           3       0.14      0.14      0.14         7\n",
    "           4       0.00      0.00      0.00         6\n",
    "           5       0.00      0.00      0.00         6\n",
    "\n",
    "    accuracy                           0.35        40\n",
    "   macro avg       0.21      0.28      0.24        40\n",
    "weighted avg       0.27      0.35      0.30        40\n",
    "\n",
    "Commentaire :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répartition des classes:\n",
      "catégories\n",
      "2    53\n",
      "1    49\n",
      "3    36\n",
      "4    31\n",
      "5    27\n",
      "Name: count, dtype: int64\n",
      "Entraînement du modèle avec Camembert + Random Forest + Oversampling...\n",
      "Random Forest + Camembert Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.90      0.67        10\n",
      "           2       0.43      0.55      0.48        11\n",
      "           3       0.33      0.29      0.31         7\n",
      "           4       0.00      0.00      0.00         6\n",
      "           5       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.42        40\n",
      "   macro avg       0.26      0.35      0.29        40\n",
      "weighted avg       0.31      0.42      0.35        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "from imblearn.pipeline import Pipeline   # Assure-toi d'utiliser imblearn.pipeline.Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Garder seulement les lignes où 'catégories' n'est pas NaN et convertir en int\n",
    "df = df.dropna(subset=['catégories'])\n",
    "df['catégories'] = df['catégories'].astype(int)\n",
    "\n",
    "# Séparation en ensemble d'entraînement et de test\n",
    "X = df[\"offre_emploi\"]\n",
    "y = df[\"catégories\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- Définition du transformeur Camembert ---\n",
    "class CamembertVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformeur qui convertit un texte en un vecteur d'embedding\n",
    "    à l'aide du modèle Camembert.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='camembert-base', device='cpu', max_length=512):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = CamembertTokenizer.from_pretrained(model_name)\n",
    "        self.model = CamembertModel.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        embeddings = []\n",
    "        for text in X:\n",
    "            inputs = self.tokenizer(\n",
    "                text,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            # Extraction de la représentation du token [CLS] (premier token)\n",
    "            emb = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "            embeddings.append(emb)\n",
    "        return np.array(embeddings)\n",
    "\n",
    "# --- Wrapper pour oversampling ---\n",
    "class OversampleTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Wrapper qui intègre un oversampler et fournit les méthodes fit et transform.\n",
    "    Pendant la phase d'entraînement, il applique fit_resample.\n",
    "    Pendant la phase de prédiction, il se contente de retourner les données telles quelles.\n",
    "    \"\"\"\n",
    "    def __init__(self, sampler):\n",
    "        self.sampler = sampler\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.X_res_, self.y_res_ = self.sampler.fit_resample(X, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Pour la phase de prédiction, on renvoie simplement X\n",
    "        return X\n",
    "\n",
    "# Définir le device (GPU si disponible)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# --- Construction du pipeline avec oversampling et Random Forest ---\n",
    "pipeline_camembert_rf = Pipeline([\n",
    "    ('camembert', CamembertVectorizer(device=device)),\n",
    "    ('oversample', OversampleTransformer(RandomOverSampler(random_state=42))),\n",
    "    ('clf', RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=10,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Entraînement\n",
    "print(\"Entraînement du modèle avec Camembert + Random Forest + Oversampling...\")\n",
    "pipeline_camembert_rf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test et affichage du rapport\n",
    "y_pred_rf = pipeline_camembert_rf.predict(X_test)\n",
    "print(\"Random Forest + Camembert Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Résultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Filtrer pour ne conserver que les lignes où 'catégories' n'est pas NaN (les 200 offres classées)\n",
    "df = df.dropna(subset=['catégories'])\n",
    "# Convertir en int si besoin (valeurs attendues de 1 à 5)\n",
    "df['catégories'] = df['catégories'].astype(int)\n",
    "\n",
    "print(\"Répartition des classes:\")\n",
    "print(df['catégories'].value_counts())\n",
    "\n",
    "# Séparation en ensemble d'entraînement et de test (80%/20% avec stratification)\n",
    "X = df[\"offre_emploi\"]\n",
    "y = df[\"catégories\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- Définition du transformeur Camembert ---\n",
    "class CamembertVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformeur scikit-learn qui convertit un texte en un vecteur d'embedding\n",
    "    à l'aide du modèle Camembert.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='camembert-base', device='cpu', max_length=512):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = CamembertTokenizer.from_pretrained(model_name)\n",
    "        self.model = CamembertModel.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        embeddings = []\n",
    "        for text in X:\n",
    "            # Tokenisation avec troncature et padding\n",
    "            inputs = self.tokenizer(\n",
    "                text,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "            # Déplacer les tenseurs sur le device (CPU ou GPU)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            # Extraction de la représentation du token [CLS] (premier token)\n",
    "            emb = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "            embeddings.append(emb)\n",
    "        return np.array(embeddings)\n",
    "\n",
    "# Définition du device : utiliser \"cuda\" si un GPU est disponible, sinon \"cpu\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# --- Construction du pipeline Camembert avec XGBoost ---\n",
    "pipeline_camembert_xgb = Pipeline([\n",
    "    ('camembert', CamembertVectorizer(device=device)),\n",
    "    ('clf', XGBClassifier(\n",
    "          n_estimators=200,\n",
    "          max_depth=6,\n",
    "          learning_rate=0.1,\n",
    "          subsample=0.8,\n",
    "          colsample_bytree=0.8,\n",
    "          random_state=42,\n",
    "          objective='multi:softprob',  # Permet d'obtenir des probabilités pour chaque classe\n",
    "          use_label_encoder=False\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Entraînement du modèle\n",
    "print(\"Entraînement du modèle avec Camembert vectorization et XGBoost...\")\n",
    "pipeline_camembert_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test et affichage du rapport de classification\n",
    "y_pred_xgb = pipeline_camembert_xgb.predict(X_test)\n",
    "print(\"Camembert-based Classification Report (XGBoost):\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Résultats\n",
    "\n",
    "Commentaire :"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
